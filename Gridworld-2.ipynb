{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%pylab inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld 2\n",
    "To make the problem more interesting, I created a 2nd grid which has more interesting structure as shown below\n",
    "<img src=\"fig5.png\"\n",
    "\n",
    "The end state is the grey cell. Transitions to the  black cells have a negative reward of -10. All other transitions have a reward of -1, while the end state has a reward of 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##2a. Bellman Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1 # discounting rate\n",
    "gridSize = 4\n",
    "\n",
    "terminationStates = [[0,0]]\n",
    "#terminationStates = [[0,0]]\n",
    "actions = [[-1, 0], [1, 0], [0, 1], [0, -1]]\n",
    "numIterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1., -10., -10., -10.],\n",
       "       [ -1.,  -1.,  -1.,  -1.],\n",
       "       [-10., -10., -10.,  -1.],\n",
       "       [ -1.,  -1.,  -1.,  -1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewardValue = np.zeros((gridSize,gridSize)) -1\n",
    "rewardValue[0]=np.array([-1,-10,-10,-10])\n",
    "rewardValue[2]=np.array([-10,-10,-10,-1])\n",
    "rewardValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actionValue(initialPosition,action):\n",
    "    if initialPosition in terminationStates:\n",
    "        finalPosition = initialPosition\n",
    "        reward=0\n",
    "    else:\n",
    "        #Compute final position\n",
    "        finalPosition = np.array(initialPosition) + np.array(action)\n",
    "        \n",
    "        # If the action moves the finalPosition out of the grid, stay in same cell\n",
    "        if -1 in finalPosition or gridSize in finalPosition:\n",
    "                finalPosition = initialPosition\n",
    "                reward= rewardValue[finalPosition[0],finalPosition[1]]\n",
    "        else:\n",
    "                reward= rewardValue[finalPosition[0],finalPosition[1]]\n",
    "    \n",
    "    #print(finalPosition)\n",
    "    return finalPosition, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valueMap = np.zeros((gridSize, gridSize))\n",
    "valueMap1 = np.zeros((gridSize, gridSize))\n",
    "states = [[i, j] for i in range(gridSize) for j in range(gridSize)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(numIterations,gamma,theta,valueMap):\n",
    "    for i in range(numIterations):\n",
    "        delta=0\n",
    "        #print(\"iterations=\",i)\n",
    "        for state in states:\n",
    "            weightedRewards=0\n",
    "            for action in actions:\n",
    "                finalPosition,reward = actionValue(state,action)\n",
    "                #print(\"reward=\",reward,\"valueMap=\",valueMap[finalPosition[0],finalPosition][1])\n",
    "                weightedRewards += 1/4* (reward + gamma * valueMap[finalPosition[0],finalPosition][1])\n",
    "            #print(weightedRewards)\n",
    "            valueMap1[state[0],state[1]]=weightedRewards\n",
    "            #print(\"wr=\",weightedRewards,\"va=\",valueMap[state[0],state[1]]) \n",
    "            delta =max(delta,abs(weightedRewards-valueMap[state[0],state[1]]))\n",
    "        valueMap = np.copy(valueMap1)\n",
    "        #print(valueMap1)\n",
    "        if(delta < 0.01):\n",
    "            print(delta)                                                   \n",
    "            print(valueMap)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009862596190146178\n",
      "[[   0.         -137.28514189 -209.19560831 -239.01378395]\n",
      " [-129.2494276  -180.67825796 -220.31626448 -237.86482779]\n",
      " [-194.08846546 -213.88769305 -231.5579035  -241.29920147]\n",
      " [-217.15664109 -227.25768494 -237.76348718 -241.51200989]]\n"
     ]
    }
   ],
   "source": [
    "valueMap = np.zeros((gridSize, gridSize))\n",
    "valueMap1 = np.zeros((gridSize, gridSize))\n",
    "states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "policy_evaluation(1000,1,0.0001,valueMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2b. Greedify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valueMap = np.zeros((gridSize, gridSize))\n",
    "valueMap1 = np.zeros((gridSize, gridSize))\n",
    "states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "pi = np.ones((gridSize,gridSize))/4\n",
    "pi1 = np.chararray((gridSize, gridSize))\n",
    "pi1[:] = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the value state function for the Grid\n",
    "def policy_evaluate(states,actions,gamma,valueMap):\n",
    "    #print(\"iterations=\",i)\n",
    "    for state in states:\n",
    "        weightedRewards=0\n",
    "        for action in actions:\n",
    "            finalPosition,reward = actionValue(state,action)\n",
    "            weightedRewards += 1/4* (reward + gamma * valueMap[finalPosition[0],finalPosition][1])\n",
    "        # Set the computed weighted rewards to valueMap1\n",
    "        valueMap1[state[0],state[1]]=weightedRewards\n",
    "    # Copy to original valueMap\n",
    "    valueMap = np.copy(valueMap1)\n",
    "    return(valueMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(q_values):\n",
    "    idx=np.argmax(q_values)\n",
    "    return(np.random.choice(np.where(a==a[idx])[0].tolist()))\n",
    "\n",
    "\n",
    "# Compute the best action in each state\n",
    "def greedify_policy(state,pi,pi1,gamma,valueMap):  \n",
    "        q_values=np.zeros(len(actions))\n",
    "        for idx,action in enumerate(actions):\n",
    "            finalPosition,reward = actionValue(state,action)\n",
    "            q_values[idx] += 1/4* (reward + gamma * valueMap[finalPosition[0],finalPosition][1])\n",
    "        # Find the index of the action for which the q_value is \n",
    "        idx=q_values.argmax()\n",
    "        pi[state[0],state[1]]=idx \n",
    "        if(idx == 0):\n",
    "            pi1[state[0],state[1]]='u'\n",
    "        elif(idx == 1):\n",
    "            pi1[state[0],state[1]]='d'\n",
    "        elif(idx == 2):\n",
    "            pi1[state[0],state[1]]='r'\n",
    "        elif(idx == 3):\n",
    "            pi1[state[0],state[1]]='l'\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(pi, pi1,gamma,valueMap):\n",
    "    policy_stable = True\n",
    "    for state in states:\n",
    "        old = pi[state].copy()\n",
    "        # Greedify policy for state\n",
    "        greedify_policy(state,pi,pi1,gamma,valueMap)\n",
    "        if not np.array_equal(pi[state], old):\n",
    "            policy_stable = False\n",
    "    print(pi)\n",
    "    print(pi1)\n",
    "    return pi, pi1, policy_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(gamma, theta):\n",
    "    valueMap = np.zeros((gridSize, gridSize))\n",
    "    pi = np.ones((gridSize,gridSize))/4\n",
    "    pi1 = np.chararray((gridSize, gridSize))\n",
    "    pi1[:] = 'a'\n",
    "    policy_stable = False\n",
    "    print(\"here\")\n",
    "    while not policy_stable:\n",
    "        valueMap = policy_evaluate(states,actions,gamma,valueMap)\n",
    "        pi,pi1, policy_stable = improve_policy(pi,pi1,  gamma,valueMap)\n",
    "    return valueMap, pi,pi1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "[[0. 3. 1. 1.]\n",
      " [0. 3. 2. 1.]\n",
      " [0. 1. 1. 1.]\n",
      " [1. 1. 2. 1.]]\n",
      "[[b'u' b'l' b'd' b'd']\n",
      " [b'u' b'l' b'r' b'd']\n",
      " [b'u' b'd' b'd' b'd']\n",
      " [b'd' b'd' b'r' b'd']]\n",
      "[[0. 3. 1. 1.]\n",
      " [0. 3. 2. 1.]\n",
      " [0. 1. 1. 1.]\n",
      " [1. 2. 2. 1.]]\n",
      "[[b'u' b'l' b'd' b'd']\n",
      " [b'u' b'l' b'r' b'd']\n",
      " [b'u' b'd' b'd' b'd']\n",
      " [b'd' b'r' b'r' b'd']]\n",
      "[[0. 3. 1. 1.]\n",
      " [0. 3. 2. 1.]\n",
      " [0. 1. 1. 1.]\n",
      " [2. 2. 2. 1.]]\n",
      "[[b'u' b'l' b'd' b'd']\n",
      " [b'u' b'l' b'r' b'd']\n",
      " [b'u' b'd' b'd' b'd']\n",
      " [b'r' b'r' b'r' b'd']]\n",
      "[[0. 3. 1. 1.]\n",
      " [0. 3. 2. 1.]\n",
      " [0. 1. 1. 1.]\n",
      " [2. 2. 2. 1.]]\n",
      "[[b'u' b'l' b'd' b'd']\n",
      " [b'u' b'l' b'r' b'd']\n",
      " [b'u' b'd' b'd' b'd']\n",
      " [b'r' b'r' b'r' b'd']]\n"
     ]
    }
   ],
   "source": [
    "theta=0.1\n",
    "valueMap, pi,pi1 = policy_iteration(gamma, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2c. Bellman Optimality update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1 # discounting rate\n",
    "rewardValue = np.zeros((gridSize,gridSize)) -1\n",
    "rewardValue[0]=np.array([-1,-10,-10,-10])\n",
    "rewardValue[2]=np.array([-10,-10,-10,-1])\n",
    "rewardValue\n",
    "gridSize = 4\n",
    "terminationStates = [[0,0]]\n",
    "actions = [[-1, 0], [1, 0], [0, 1], [0, -1]]\n",
    "numIterations = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valueMap = np.zeros((gridSize, gridSize))\n",
    "valueMap1 = np.zeros((gridSize, gridSize))\n",
    "states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "pi = np.ones((gridSize,gridSize))/4\n",
    "pi1 = np.chararray((gridSize, gridSize))\n",
    "pi1[:] = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_optimality_update(valueMap, state, gamma):\n",
    "\n",
    "    q_values=np.zeros(len(actions))\n",
    "    \n",
    "    for idx,action in enumerate(actions):\n",
    "        finalPosition,reward = actionValue(state,action)\n",
    "        q_values[idx] += 1/4* (reward + gamma * valueMap[finalPosition[0],finalPosition][1])\n",
    "    # Find the index of the action for which the q_value is \n",
    "    idx=q_values.argmax()\n",
    "            \n",
    "    max = np.argmax(q_values)\n",
    "    valueMap[state[0],state[1]] = q_values[max]    \n",
    "    #print(q_values[max])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(gamma, theta):\n",
    "    valueMap = np.zeros((gridSize, gridSize))\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in states:\n",
    "            v_old=valueMap[state[0],state[1]]\n",
    "            bellman_optimality_update(valueMap, state, gamma)\n",
    "            delta = max(delta, abs(v_old - valueMap[state[0],state[1]]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    pi = np.ones((gridSize,gridSize))/4\n",
    "    for state in states:\n",
    "        greedify_policy(state,pi,pi1,gamma,valueMap)\n",
    "    print(pi)\n",
    "    print(pi1)\n",
    "    return valueMap, pi,pi1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 3. 1. 1.]\n",
      " [0. 3. 3. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [2. 2. 2. 0.]]\n",
      "[[b'u' b'l' b'd' b'd']\n",
      " [b'u' b'l' b'l' b'l']\n",
      " [b'u' b'u' b'u' b'u']\n",
      " [b'r' b'r' b'r' b'u']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "chararray([[b'u', b'l', b'd', b'd'],\n",
       "           [b'u', b'l', b'l', b'l'],\n",
       "           [b'u', b'u', b'u', b'u'],\n",
       "           [b'r', b'r', b'r', b'u']], dtype='|S1')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 1\n",
    "theta = 0.000001\n",
    "valueMap,pi,pi1=value_iteration(gamma, theta)\n",
    "pi\n",
    "pi1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
