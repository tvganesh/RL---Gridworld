{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "Reinforcement Learning (RL) involves decision making under uncertainty which tries to maximize return over successive states.There are four main elements of a Reinforcement Learning system: a policy, a reward signal, a value function. The policy is a mapping from the states to actions or a probability distribution of actions. Every action the agent takes results in a numerical reward. The agent's sole purpose is to maximize the reward in the long run.\n",
    "\n",
    "Reinforcement Learning is very different from Supervised, Unsupervised and Semi-supervised learning where the data is either labeled, unlabeled or paritially labeled and the learning algorithm tries to learn the target values from the input features which is then used either for inference or prediction. In unsupervised the intention is to extract patterns from the data. In Reinforcement Learning the agent/robot takes action in each state based on the reward it would get for a particular action in a specific state with the goal of maximizing the reward. In many ways is similar to how human beings and animals learn. Every action we take is either with the goal of increasing our overall happiness, contentment, money,fame, power over the opposite.\n",
    "\n",
    "RL has been used very effectively in many situations, the most famous is AlphaGo from Deep Mind, the first computer program to defeat a professional Go player in the Go game, which is supposed to extremely complex.\n",
    "Also AlphaZero, from DeepMind has a higher ELO rating that Stockfish and was able to beat Stockfish 800+ times in 1000 matches.\n",
    "Take a look at [DeepMind](https://deepmind.com/)\n",
    "\n",
    "In this post, I use some of the basic concepts of Reinforcment Learning to solve Grids (mazes). With this we can solve mazes, with arbitrary size, shape and complexity fairly easily. The RL algorithm can find the optimal path through the maze. \n",
    "Incidentally, I recollect recursive algorithms in Data Structures book which take a much more complex route with a lot of back tracking to solve maze problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "Reinforcement Learning involves decision making under uncertainty which tries to maximize return over successive states.There are four main elements of a Reinforcement Learning system: a policy, a reward signal, a value function. The policy is a mapping from the states to actions or a probability distribution of actions. Every action the agent takes results in a numerical reward. The agent's sole purpose is to maximize the reward in the long run.\n",
    "\n",
    "The reward indicates the immediate return, a value function specifies the return in the  long run. Value of a state is the expected reward that an agent can accrue.\n",
    "\n",
    "The agent/robot takes an action in At in state St and moves to state S't anf gets a reward Rt+1 as shown\n",
    "<img src=\"fig1.png\">\n",
    "\n",
    "An agent will seek to maximize the overall return as it transition across states\n",
    "The expected return can be expressed as\n",
    "$G_{t} = R_{t+1} + \\gamma G_{t+1}$ where $G_{t}$ is the expected return in time t and the discounted expected return $G_{t+1}$ in time t+1\n",
    "\n",
    "A policy is a mapping from states to probabilities of selecting each possible action. If the agent is following policy $\\pi$ at time t, then $\\pi(a|s)$ is the probability that $A_{t}$ = a if $S_{t}$ = s.\n",
    "\n",
    "The value function of a state s under a policy $\\pi$, denoted $v_{\\pi}(s)$, is the expected return when starting in s and following $\\pi$ thereafter\n",
    "\n",
    "This can be written as\n",
    "\n",
    "$v_{\\pi}(s) = E_{\\pi}[G_{t} |S_{t}=s] = E_{\\pi}[\\sum_{k=0}^{k=Inf} \\gamma^{k}R_{t+k+1}|S_{t}=s]$\n",
    "\n",
    "= $E_{\\pi}[R_{t+1} + \\gamma G_{t+1} |S_{t}=s]$\n",
    "\n",
    "$v_{\\pi}(s)=\\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a)[r+\\gamma*v_{\\pi}(s')]$\n",
    "\n",
    "Similarly the action value function gives the expected return when taking an action 'a' in state 's'\n",
    "$q_{\\pi}(s,a)= \\sum_{s',r} p(s',r|s,a)[r+\\gamma*\\pi(a|s)q_{\\pi}(s',a')]$\n",
    "\n",
    "These are Bellman's equation for the state value function\n",
    "\n",
    "The Bellman equations give the equation for each of the state\n",
    "\n",
    "The Bellman optimality equations give the optimal policy of choosing specific actions in specific states to achieve the maximim reward and reach the goal efficiently. They are given as\n",
    "\n",
    "$v_{*}(s)=max_{a}\\sum_{s',r} p(s',r|s,a)[r+\\gamma*v_{*}(s')]$\n",
    "\n",
    "$q_{*}(s,a)=\\sum_{s',r} p(s',r|s,a)[r+\\gamma*max_{a}q_{*}(s',a')]$\n",
    "\n",
    "The Bellman equations cannot be used directly in goal directed problems and dynamic programming is used instead where the value functions are computed iteratively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridworld\n",
    "In this post I solve Grids using Reinforcement Learning. In the problem below the Maze has 2 end states as shown in the corner. There are four possible actions in each state up, down, right and left. If an action in a state takes it out of the grid then the agent remains in the same state. All actions have a reward of -1 while the end states have a reward of 0\n",
    "\n",
    "This is shown as <img src=\"fig2.png\">\n",
    "\n",
    "where the reward for any transition is $R_{t}=-1$ except the transition to the end states at the corner which have a reward of 0. The policy is a uniform policy with all actions being equi-probable with a probability of 1/4 or 0.25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid world -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1 # discounting rate\n",
    "gridSize = 4\n",
    "rewardValue = -1\n",
    "terminationStates = [[0,0], [gridSize-1, gridSize-1]]\n",
    "actions = [[-1, 0], [1, 0], [0, 1], [0, -1]]\n",
    "numIterations = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action value provides the next state for a given action in a state and the accrued reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actionValue(initialPosition,action):\n",
    "    if initialPosition in terminationStates:\n",
    "        finalPosition = initialPosition\n",
    "        reward=0\n",
    "    else:\n",
    "        #Compute final position\n",
    "        finalPosition = np.array(initialPosition) + np.array(action)\n",
    "        reward= rewardValue\n",
    "    # If the action moves the finalPosition out of the grid, stay in same cell\n",
    "    if -1 in finalPosition or gridSize in finalPosition:\n",
    "        finalPosition = initialPosition\n",
    "        reward= rewardValue\n",
    "    \n",
    "    #print(finalPosition)\n",
    "    return finalPosition, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize valueMap and valueMap1\n",
    "valueMap = np.zeros((gridSize, gridSize))\n",
    "valueMap1 = np.zeros((gridSize, gridSize))\n",
    "states = [[i, j] for i in range(gridSize) for j in range(gridSize)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(numIterations,gamma,theta,valueMap):\n",
    "    for i in range(numIterations):\n",
    "        delta=0\n",
    "        for state in states:\n",
    "            weightedRewards=0\n",
    "            for action in actions:\n",
    "                finalPosition,reward = actionValue(state,action)\n",
    "                weightedRewards += 1/4* (reward + gamma * valueMap[finalPosition[0],finalPosition][1])\n",
    "            valueMap1[state[0],state[1]]=weightedRewards\n",
    "            delta =max(delta,abs(weightedRewards-valueMap[state[0],state[1]]))\n",
    "        valueMap = np.copy(valueMap1)\n",
    "        if(delta < 0.01):                                                \n",
    "            print(valueMap)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.         -13.89528403 -19.84482978 -21.82635535]\n",
      " [-13.89528403 -17.86330422 -19.84586777 -19.84482978]\n",
      " [-19.84482978 -19.84586777 -17.86330422 -13.89528403]\n",
      " [-21.82635535 -19.84482978 -13.89528403   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "valueMap = np.zeros((gridSize, gridSize))\n",
    "valueMap1 = np.zeros((gridSize, gridSize))\n",
    "states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "policy_evaluation(1000,1,0.001,valueMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings\n",
    "The valueMap is the result of several sweeps through all the states. It can be seen that the cells in the corner state have a higher value. We can start on any cell in the grid and move in the direction which is greater than the current state and we will reach the end state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedify\n",
    "\n",
    "The previous alogirthm while it works is somewhat inefficient as we have to sweep over the states to compute the state value function. The approach below works on the same problem but after each computation of the value function, a greedifications takes place to ensure that the action with the highest return is selected after which the policy $\\pi$ is followed\n",
    "\n",
    "To make the transitions clearer I also create another grid which shows the path from any cell to the end states as\n",
    "\n",
    "'u' - up\n",
    "\n",
    "'d' - down\n",
    "\n",
    "'r' - right\n",
    "\n",
    "'l' - left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valueMap = np.zeros((gridSize, gridSize))\n",
    "valueMap1 = np.zeros((gridSize, gridSize))\n",
    "states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "pi = np.ones((gridSize,gridSize))/4\n",
    "pi1 = np.chararray((gridSize, gridSize))\n",
    "pi1[:] = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the value state function for the Grid\n",
    "def policy_evaluate(states,actions,gamma,valueMap):\n",
    "    #print(\"iterations=\",i)\n",
    "    for state in states:\n",
    "        weightedRewards=0\n",
    "        for action in actions:\n",
    "            finalPosition,reward = actionValue(state,action)\n",
    "            weightedRewards += 1/4* (reward + gamma * valueMap[finalPosition[0],finalPosition][1])\n",
    "        # Set the computed weighted rewards to valueMap1\n",
    "        valueMap1[state[0],state[1]]=weightedRewards\n",
    "    # Copy to original valueMap\n",
    "    valueMap = np.copy(valueMap1)\n",
    "    return(valueMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(q_values):\n",
    "    idx=np.argmax(q_values)\n",
    "    return(np.random.choice(np.where(a==a[idx])[0].tolist()))\n",
    "\n",
    "\n",
    "# Compute the best action in each state\n",
    "def greedify_policy(state,pi,pi1,gamma,valueMap):  \n",
    "        q_values=np.zeros(len(actions))\n",
    "        for idx,action in enumerate(actions):\n",
    "            finalPosition,reward = actionValue(state,action)\n",
    "            q_values[idx] += 1/4* (reward + gamma * valueMap[finalPosition[0],finalPosition][1])\n",
    "        # Find the index of the action for which the q_value is \n",
    "        idx=q_values.argmax()\n",
    "        pi[state[0],state[1]]=idx \n",
    "        if(idx == 0):\n",
    "            pi1[state[0],state[1]]='u'\n",
    "        elif(idx == 1):\n",
    "            pi1[state[0],state[1]]='d'\n",
    "        elif(idx == 2):\n",
    "            pi1[state[0],state[1]]='r'\n",
    "        elif(idx == 3):\n",
    "            pi1[state[0],state[1]]='l'\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(pi, pi1,gamma,valueMap):\n",
    "    policy_stable = True\n",
    "    for state in states:\n",
    "        old = pi[state].copy()\n",
    "        # Greedify policy for state\n",
    "        greedify_policy(state,pi,pi1,gamma,valueMap)\n",
    "        if not np.array_equal(pi[state], old):\n",
    "            policy_stable = False\n",
    "    print(pi)\n",
    "    print(pi1)\n",
    "    return pi, pi1, policy_stable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(gamma, theta):\n",
    "    valueMap = np.zeros((gridSize, gridSize))\n",
    "    pi = np.ones((gridSize,gridSize))/4\n",
    "    pi1 = np.chararray((gridSize, gridSize))\n",
    "    pi1[:] = 'a'\n",
    "    policy_stable = False\n",
    "    print(\"here\")\n",
    "    while not policy_stable:\n",
    "        valueMap = policy_evaluate(states,actions,gamma,valueMap)\n",
    "        pi,pi1, policy_stable = improve_policy(pi,pi1,  gamma,valueMap)\n",
    "    return valueMap, pi,pi1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "[[0. 3. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 2. 0.]]\n",
      "[[b'u' b'l' b'u' b'u']\n",
      " [b'u' b'u' b'u' b'u']\n",
      " [b'u' b'u' b'u' b'd']\n",
      " [b'u' b'u' b'r' b'u']]\n",
      "[[0. 3. 3. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 2. 2. 0.]]\n",
      "[[b'u' b'l' b'l' b'u']\n",
      " [b'u' b'u' b'u' b'd']\n",
      " [b'u' b'u' b'd' b'd']\n",
      " [b'u' b'r' b'r' b'u']]\n",
      "[[0. 3. 3. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 2. 2. 0.]]\n",
      "[[b'u' b'l' b'l' b'd']\n",
      " [b'u' b'u' b'd' b'd']\n",
      " [b'u' b'u' b'd' b'd']\n",
      " [b'u' b'r' b'r' b'u']]\n",
      "[[0. 3. 3. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 2. 2. 0.]]\n",
      "[[b'u' b'l' b'l' b'd']\n",
      " [b'u' b'u' b'd' b'd']\n",
      " [b'u' b'u' b'd' b'd']\n",
      " [b'u' b'r' b'r' b'u']]\n"
     ]
    }
   ],
   "source": [
    "theta=0.1\n",
    "valueMap, pi,pi1 = policy_iteration(gamma, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings\n",
    "From the above valueMap we can see that greedification solves this much faster as below\n",
    "\n",
    "<img src = \"fig3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Optimality update\n",
    "\n",
    "The Bellman optimality update directly updates the value state function for the action that results in the maximum return in a state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1 # discounting rate\n",
    "rewardValue = -1\n",
    "gridSize = 4\n",
    "terminationStates = [[0,0], [gridSize-1, gridSize-1]]\n",
    "actions = [[-1, 0], [1, 0], [0, 1], [0, -1]]\n",
    "numIterations = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valueMap = np.zeros((gridSize, gridSize))\n",
    "valueMap1 = np.zeros((gridSize, gridSize))\n",
    "states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "pi = np.ones((gridSize,gridSize))/4\n",
    "pi1 = np.chararray((gridSize, gridSize))\n",
    "pi1[:] = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_optimality_update(valueMap, state, gamma):\n",
    "\n",
    "    q_values=np.zeros(len(actions))\n",
    "    \n",
    "    for idx,action in enumerate(actions):\n",
    "        finalPosition,reward = actionValue(state,action)\n",
    "        q_values[idx] += 1/4* (reward + gamma * valueMap[finalPosition[0],finalPosition][1])\n",
    "    # Find the index of the action for which the q_value is \n",
    "    idx=q_values.argmax()\n",
    "            \n",
    "    max = np.argmax(q_values)\n",
    "    valueMap[state[0],state[1]] = q_values[max]    \n",
    "    #print(q_values[max])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(gamma, theta):\n",
    "    valueMap = np.zeros((gridSize, gridSize))\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in states:\n",
    "            v_old=valueMap[state[0],state[1]]\n",
    "            bellman_optimality_update(valueMap, state, gamma)\n",
    "            delta = max(delta, abs(v_old - valueMap[state[0],state[1]]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    pi = np.ones((gridSize,gridSize))/4\n",
    "    for state in states:\n",
    "        greedify_policy(state,pi,pi1,gamma,valueMap)\n",
    "    print(pi)\n",
    "    print(pi1)\n",
    "    return valueMap, pi,pi1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 3. 3. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 2. 2. 0.]]\n",
      "[[b'u' b'l' b'l' b'd']\n",
      " [b'u' b'u' b'u' b'd']\n",
      " [b'u' b'u' b'd' b'd']\n",
      " [b'u' b'r' b'r' b'u']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "chararray([[b'u', b'l', b'l', b'd'],\n",
       "           [b'u', b'u', b'u', b'd'],\n",
       "           [b'u', b'u', b'd', b'd'],\n",
       "           [b'u', b'r', b'r', b'u']], dtype='|S1')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 1\n",
    "theta = 0.01\n",
    "valueMap,pi,pi1=value_iteration(gamma, theta)\n",
    "pi\n",
    "pi1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings\n",
    "The above valueMap shows the optimal path from any state\n",
    "\n",
    "<img src = \"fig4.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
